---
title: |
  | Introduction to the Design and Analysis of Randomized Experiments
  | Class 4: Power Analysis
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: Jake Bowers
bibliography:
 - ../course.bib
fontsize: 10pt
graphics: yes
biblio-style: authoryear-comp
link-citations: true
biblatexoptions:
  - natbib=true
header-includes: |
  \setbeameroption{hide notes}
  \usepackage{hyperref}
  \usepackage{bookmark}
  \setbeamertemplate{footline}{\begin{beamercolorbox}{section in head/foot}
  \insertframenumber/\inserttotalframenumber \end{beamercolorbox}}
  \setbeamertemplate{itemize item}[circle]
  \setbeamertemplate{itemize subitem}{\raisebox{0.2em}{\scalebox{1}{$\blacktriangleright$}}}
  \setbeamersize{description width=2ex}
  \setbeamersize{text margin left=1ex,text margin right=1ex}
  \usepackage{tikz}
  \usepackage{tikz-cd}
  \usepackage{textpos}
  \usepackage{booktabs,multirow,makecell}
  \usepgflibrary{arrows}
  \usetikzlibrary{arrows}
  \tikzstyle{every picture}+=[remember picture]
  \newcommand{\theHtable}{\thetable}
  \usepackage{amsmath,amsthm}
output:
  beamer_presentation:
    slide_level: 2
    keep_tex: true
    toc: true
    latex_engine: lualatex
    citation_package: biblatex
    incremental: false
    md_extensions: +raw_attribute-tex_math_single_backslash+autolink_bare_uris+ascii_identifiers+tex_math_dollars
    pandoc_args: [ "--csl", "chicago-author-date.csl", "--toc" ]
colorlinks: true
---

```{r setup, include=FALSE,echo=FALSE}
# Load all the libraries we need
library(here)
library(kableExtra)
library(styler)
library(coin)
library(multcomp)
library(devtools)
library(randomizr)
library(rcompanion) ## for pairwisePermutationTest()
library(DeclareDesign)
library(estimatr)
library(tidyverse)
library(RItools)
```

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

## from https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf(
      "<span style='color: %s;'>%s</span>", color,
      x
    )
  } else {
    x
  }
}
```


```{r makedat, echo=FALSE}
## First, create some data,
##  y0 is potential outcome to control
N <- 20
set.seed(123)
dat <- data.frame(
  y0 = c(rpois(N - 2, lambda = 2), rpois(2, lambda = 100))
)
## Just messing around with heterogeneous treatment effects
dat$tau0 <- rnorm(n = N, mean = 0, sd = sd(dat$y0))
dat$tau <- ifelse(dat$tau0 < 0, 0, signif(dat$tau0, 2))
dat$T <- complete_ra(N)
dat$y1 <- with(dat, y0 + tau)
dat$Y <- with(dat, T * y1 + (1 - T) * y0)
dat$Ybin <- as.numeric(dat$Y > median(dat$Y))
## Setup for use with coin
dat$TF <- factor(dat$T)
```



# Overview and Review

##  Today

  0. Quiz and Questions
  1. Statistical Power (1 - false negative rate of tests)

Note: You can download (and contribute to) course materials at [https://github.com/bowers-illinois-edu/short_course_experiments](https://github.com/bowers-illinois-edu/short_course_experiments)

Hay recursos en espaÃ±ol aqui: <https://egap.github.io/theory_and_practice_of_field_experiments_spanish/>

## Lingering Questions?

Questions arising?

## Quiz

# What is power?

##  What is power?

We want to separate signal from noise.

- Power = probability of rejecting null hypothesis, given true effect $\ne$ 0.

- It is the ability to detect signal from noise (assuming there is a signal).

- Formally: power = (1 - Type II) error rate.

- Thus, power $\in$ (0, 1).

- Standard thresholds: 0.8 or 0.9 --- "nearly always detect a signal when one exists"

## Starting point for power analysis

- Power analysis is something we do *before* we run a study.

    - Helps you figure out the sample you need to detect a given effect size.

    - Or helps you figure out a minimal detectable difference given a set sample size.

    - May help you decide whether to run a study at all.

- It is hard to learn from an under-powered null finding.

    - Was there an effect, but we were unable to detect it? or was there no effect?  We can't say.

    - How should we interpret "The difference in proportion vaccinated between Message A and Control was .02 ($p=.4$)."?


## Power

- Say there truly is a treatment effect and you run your experiment many times
  (hypothetically) with the same group of people.  How often will you get a $p
  \le .05$?

- It depends:

    - How big is your treatment effect?

    - How many units are treated, measured?

    - How much noise is there in the measurement of your outcome?

- **We do not know the answers to all those questions in advance. So some guesswork required to answer this question.**


## Approaches to power calculation

- Analytical calculations of power

- Simulation


## Power calculation tools

- Interactive

    - [EGAP Power Calculator](https://egap.shinyapps.io/power-app/)

    - [rpsychologist](https://rpsychologist.com/d3/NHST/)

- R Packages

    - [pwr](https://cran.r-project.org/web/packages/pwr/index.html)

    - [DeclareDesign](https://cran.r-project.org/web/packages/DeclareDesign/index.html), see also <https://declaredesign.org/>


# Analytical calculations of power

## Analytical calculations of power for hypotheses about no average treatment effects

- Formula:
  \begin{align*}
  \text{Power} &= \Phi\left(\frac{|\tau| \sqrt{N}}{2\sigma}- \Phi^{-1}(1- \frac{\alpha}{2})\right)
  \end{align*}

- Components:
  - $\phi$: standard normal CDF is monotonically increasing
  - $\tau$: the effect size
  - $N$: the sample size
  - $\sigma$: the standard deviation of the outcome
  - $\alpha$: the significance level (typically 0.05)


## Example: Analytical calculations of power

```{r pwrsimp, echo=TRUE, include=TRUE}
# Power for a study with 80 obserations and effect
# size of 0.25
library(pwr)
pwr.t.test(n = 40, d = 0.25, sig.level = 0.05,
           power = NULL, type = c("two.sample",
                      "one.sample", "paired"))
```


## Limitations to analytical power calculations
- Only derived for some estimands (ATE/ITT)

- Makes specific assumptions about the data-generating process (for example, $N$ is large)

- Difficult with more complex designs like block randomized designs with different probabilities of assignment in each block.


# Simulation-based power calculation
## Simulation-based power calculation
- Create dataset and simulate research design.

- Assumptions are necessary for simulation studies, but you make your own.

- For the DeclareDesign approach, see <https://declaredesign.org/>

## Steps

- Define the sample and the potential outcomes function.

- Define the treatment assignment procedure.

- Create data.

- Assign treatment, then estimate the effect.

- Do this many times.


## Examples

- Complete randomization

- With covariates

- With cluster randomization

## Example: Simulation-based power for complete randomization

```{r powersim, echo=TRUE, include=TRUE}
# install.packages("randomizr")
library("randomizr")
library("estimatr")

## Y0 is fixed in most field experiments.
## So we only generate it once (here making it Normal parallels with analytic approaches):
make_Y0 <- function(N){  rnorm(n = N) }
repeat_experiment_and_test <- function(N, Y0, tau){
    # N is size of experimental pool; Y0 is potential outcome to control
    # tau is effect size (here, a constant additive effect)
    Z <- complete_ra(N = N)
    Y1 <- Y0 + Z * tau
    Yobs <- Z * Y1 + (1 - Z) * Y0
    estimator <- lm_robust(Yobs ~ Z)
    pval <- estimator$p.value[2]
    return(pval)
}
```

## Example: Simulation-based power for complete randomization
```{r powersim2, echo=TRUE, include=TRUE}
power_sim <- function(N,tau,sims){
    Y0 <- make_Y0(N)
    pvals <- replicate(n=sims,
      repeat_experiment_and_test(N=N,Y0=Y0,tau=tau))
    pow <- sum(pvals < .05) / sims
    return(pow)
}

set.seed(12345)
## Notice simulation variability with sims=100
power_sim(N=80,tau=.25,sims=1000)
power_sim(N=80,tau=.25,sims=1000)
```

## Example: Using DeclareDesign {.allowframebreaks}
```{r ddversion, echo=TRUE, message=FALSE, warning=FALSE}
library(DeclareDesign)
library(tidyverse)
P0 <- declare_population(N,u0=rnorm(N))
# declare Y(Z=1) and Y(Z=0)
O0 <- declare_potential_outcomes(Y_Z_0 = 5 + u0, Y_Z_1 = Y_Z_0 + tau)
# design is to assign m units to treatment
A0 <- declare_assignment(m=round(N/2))
# estimand is the average difference between Y(Z=1) and Y(Z=0)
estimand_ate <- declare_inquiry(ATE=mean(Y_Z_1 - Y_Z_0))
R0 <- declare_reveal(Y,Z)
design0_base <- P0 + A0 + O0 + R0

## For example with N=100 and tau=.25:
design0_N100_tau25 <- redesign(design0_base,N=100,tau=.25)
dat0_N100_tau25 <- draw_data(design0_N100_tau25)
head(dat0_N100_tau25)
with(dat0_N100_tau25,mean(Y_Z_1 - Y_Z_0)) # true ATE
with(dat0_N100_tau25,mean(Y[Z==1]) - mean(Y[Z==0])) # estimate
lm_robust(Y~Z,data=dat0_N100_tau25)$coef # estimate
```

```{r ddversion_sim, echo=TRUE, warnings=FALSE}
E0 <- declare_estimator(Y~Z,model=lm_robust,label="t test 1",
                        inquiry="ATE")
t_test <- function(data) {
       test <- with(data, t.test(x = Y[Z == 1], y = Y[Z == 0]))
       data.frame(statistic = test$statistic, p.value = test$p.value)
}
T0 <- declare_test(handler=label_test(t_test),label="t test 2")
design0_plus_tests <- design0_base + E0 + T0

design0_N100_tau25_plus <- redesign(design0_plus_tests,N=100,tau=.25)

## Only repeat the random assignment, not the creation of Y0. Ignore warning
names(design0_N100_tau25_plus)
design0_N100_tau25_sims <- simulate_design(design0_N100_tau25_plus,
          sims=c(1,100,1,1,1,1)) # only repeat the random assignment
# design0_N100_tau25_sims has 200 rows (2 tests * 100 random assignments)
# just look at the first 6 rows
head(design0_N100_tau25_sims)
```

```{r powsum}
# for each estimator, power = proportion of simulations with p.value < 0.5
design0_N100_tau25_sims %>% group_by(estimator_label) %>%
  summarize(pow=mean(p.value < .05),.groups="drop")
```

# Power with covariate adjustment
## Covariate adjustment and power

   - Covariate/Covariance adjustment can improve power because it mops up variation in the outcome variable.

      - If prognostic (predictive of the outcome), covariate adjustment can reduce variance dramatically.  Lower outcome variance means higher power.

      - If non-prognostic, power gains are minimal.

   - All covariates must be pre-treatment.  Do not drop observations on account of missingness.

      - See the   [Theory and Practice module on threats to internal validity](https://egap.github.io/theory_and_practice_of_field_experiments/threats-to-the-internal-validity-of-randomized-experiments.html) and the [10 things to know about covariate adjustment](https://egap.org/resource/10-things-to-know-about-covariate-adjustment).

  - @freedman2008rae pointed out that covariance-adjusted estimators of the ATE are biased.
  . @lin_agnostic_2013 shows that bias decreases with N.

<!-- ## Covariate adjustment: best practices -->
<!-- - All covariates must be pre-treatment -->
<!--   - Never adjust for post-treatment variables -->
<!-- - In practice, if all controls are pre-treatment, you can add whatever controls you want -->
<!--   - But there is a limit to the number -->
<!--   - Also see -->
<!-- - Missingness in pre-treatment covariates -->
<!--   - Do not drop observations on account of pre-treatment missingness -->
<!--   - Impute mean/median for pretreatment variable -->
<!--   - Include missingness indicator and impute some value in the missing variable -->



## Blocking
- Blocking: randomly assign treatment within blocks

   - âEx-anteâ covariate adjustment

   - Higher precision/efficiency implies more power

   - Reduce âconditional biasâ: association between treatment assignment and potential outcomes

   - Benefits of blocking over covariate adjustment clearest in small experiments


## Example: Simulation-based power with a covariate {.allowframebreaks}
```{r powsimcov, echo=TRUE}
## Y0 is fixed in most field experiments. So we only generate it once
make_Y0_cov <- function(N){
    u0 <- rnorm(n = N)
    x <- rpois(n=N,lambda=2)
    Y0 <- .5 * sd(u0) * x + u0
    return(data.frame(Y0=Y0,x=x))
 }
##  X is moderarely predictive of Y0.
test_dat <- make_Y0_cov(100)
test_lm  <- lm_robust(Y0~x,data=test_dat)
summary(test_lm)

## now set up the simulation
repeat_experiment_and_test_cov <- function(N, tau, Y0, x){
    Z <- complete_ra(N = N)
    Y1 <- Y0 + Z * tau
    Yobs <- Z * Y1 + (1 - Z) * Y0
    estimator <- lm_robust(Yobs ~ Z+x,data=data.frame(Y0,Z,x))
    pval <- estimator$p.value[2]
    return(pval)
}
## create the data once, randomly assign treatment sims times
## report what proportion return p-value < 0.05
power_sim_cov <- function(N,tau,sims){
    dat <- make_Y0_cov(N)
    pvals <- replicate(n=sims, repeat_experiment_and_test_cov(N=N,
                          tau=tau,Y0=dat$Y0,x=dat$x))
    pow <- sum(pvals < .05) / sims
    return(pow)
}
```

```{r, echo=TRUE}
set.seed(12345)
power_sim_cov(N=80,tau=.25,sims=100)
power_sim_cov(N=80,tau=.25,sims=100)

```

# Power for cluster randomization
## Power and clustered designs

- Given a fixed $N$, a clustered design is weakly less powered than a non-clustered design.
    - The difference is often substantial.

- We have to estimate variance correctly:
    - Clustering standard errors (the usual)
    - Randomization inference

- To increase power:
    - Better to increase number of clusters than number of units per cluster.
    - How much clusters reduce power depends critically on the intra-cluster correlation (the ratio of variance within clusters to total variance).


## A note on clustering in observational research
- Often overlooked, leading to (possibly) wildly understated uncertainty.

  - Frequentist inference based on ratio $\hat{\beta}/\hat{se}$

  - If we underestimate $\hat{se}$, we are much more likely to reject $H_0$. (Type-I error rate is too high.)

- Many observational designs much less powered than we think they are.

## Example: Simulation-based power for cluster randomization {.allowframebreaks}

```{r powsimclus, echo=TRUE}
## Y0 is fixed in most field experiments. So we only generate it once
make_Y0_clus <- function(n_indivs,n_clus){
    # n_indivs in number of people per cluster
    # n_clus is number of clusters
    clus_id <- gl(n_clus,n_indivs)
    N <- n_clus * n_indivs
    u0 <- fabricatr::draw_normal_icc(N=N,clusters=clus_id,ICC=.1)
    Y0 <- u0
    return(data.frame(Y0=Y0,clus_id=clus_id))
 }

test_dat <- make_Y0_clus(n_indivs=10,n_clus=100)
# confirm that this produces data with 10 in each of 100 clusters
table(test_dat$clus_id)
# confirm ICC
ICC::ICCbare(y=Y0,x=clus_id,data=test_dat)


repeat_experiment_and_test_clus <- function(N, tau, Y0, clus_id){
   # here we randomize Z at the cluster level
    Z <- cluster_ra(clusters=clus_id)
    Y1 <- Y0 + Z * tau
    Yobs <- Z * Y1 + (1 - Z) * Y0
    estimator <- lm_robust(Yobs ~ Z, clusters = clus_id,
                    data=data.frame(Y0,Z,clus_id), se_type = "CR2")
    pval <- estimator$p.value[2]
    return(pval)
  }
power_sim_clus <- function(n_indivs,n_clus,tau,sims){
    dat <- make_Y0_clus(n_indivs,n_clus)
    N <- n_indivs * n_clus
    # randomize treatment sims times
    pvals <- replicate(n=sims,
                repeat_experiment_and_test_clus(N=N,tau=tau,
                                Y0=dat$Y0,clus_id=dat$clus_id))
    pow <- sum(pvals < .05) / sims
    return(pow)
}
```

```{r, echo=TRUE}
set.seed(12345)
power_sim_clus(n_indivs=8,n_clus=100,tau=.25,sims=100)
power_sim_clus(n_indivs=8,n_clus=100,tau=.25,sims=100)

```

## Example: Simulation-based power for cluster randomization (DeclareDesign) {.allowframebreaks}

```{r ddversion_clus, echo=TRUE}

P1 <- declare_population(N = n_clus * n_indivs,
    clusters=gl(n_clus,n_indivs),
    u0=draw_normal_icc(N=N,clusters=clusters,ICC=.2))
O1 <- declare_potential_outcomes(Y_Z_0 = 5 + u0, Y_Z_1 = Y_Z_0 + tau)
A1 <- declare_assignment(clusters=clusters)
estimand_ate <- declare_inquiry(ATE=mean(Y_Z_1 - Y_Z_0))
R1 <- declare_reveal(Y,Z)
design1_base <- P1 + A1 + O1 + R1 + estimand_ate

## For example:
design1_test <- redesign(design1_base,n_clus=10,n_indivs=100,tau=.25)
test_d1 <- draw_data(design1_test)
# confirm all individuals in a cluster have the same treatment assignment
with(test_d1,table(Z,clusters))
# four estimators, differ in se_type:
E1a <- declare_estimator(Y~Z,model=lm_robust,clusters=clusters,
                         se_type="CR2", label="CR2 cluster t test",
                         inquiry="ATE")
E1b <- declare_estimator(Y~Z,model=lm_robust,clusters=clusters,
                         se_type="CR0", label="CR0 cluster t test",
                         inquiry="ATE")
E1c <- declare_estimator(Y~Z,model=lm_robust,clusters=clusters,
                         se_type="stata", label="stata RCSE t test",
                         inquiry="ATE")
E1d <- declare_estimator(Y~Z,model=lm_robust,
                         se_type="classical", label="plain IID OLS t test",
                         inquiry="ATE")

design1_plus <- design1_base + E1a + E1b + E1c + E1d

design1_plus_tosim <- redesign(design1_plus,n_clus=10,n_indivs=100,tau=.25)
```

## Example: Simulation-based power for cluster randomization (DeclareDesign) {.allowframebreaks}

```{r ddversion_clus_sims, echo=TRUE, cache=TRUE, warnings=FALSE}
## Only repeat the random assignment, not the creation of Y0. Ignore warning
## We would want more simulations in practice.
set.seed(12355)
design1_sims <- simulate_design(design1_plus_tosim,
                    sims=c(1,1000,rep(1,length(design1_plus_tosim)-2)))
design1_sims %>% group_by(estimator_label) %>%
  summarize(pow=mean(p.value < .05),
    coverage = mean(estimand <= conf.high & estimand >= conf.low),
    .groups="drop")

```

## Example: Simulation-based power for cluster randomization (DeclareDesign) {.allowframebreaks}

```{r ddversion_clus_2, echo=TRUE, cache=TRUE, results='hide', warnings=FALSE}
## This may be simpler than the above:
d1 <- block_cluster_two_arm_designer(N_blocks = 1,
    N_clusters_in_block = 10,
    N_i_in_cluster = 100,
    sd_block = 0,
    sd_cluster = .3,
    ate = .25)
d1_plus <- d1 + E1b + E1c + E1d
d1_sims <- simulate_design(d1_plus,sims=c(1,1,1000,1,1,1,1,1,1))
```

```{r ddversion_clus_2_print, echo=TRUE}
d1_sims %>% group_by(estimator_label) %>% summarize(pow=mean(p.value < .05),
    coverage = mean(estimand <= conf.high & estimand >= conf.low),
    .groups="drop")
```

# Power Ingredients and Their Relationship

## Comparative Statics
- Power is:
  - Increasing in $N$
  - Increasing in $|\tau|$
  - Decreasing in $\sigma$

## Power by sample size {.allowframebreaks}

```{r powplot_by_n, echo=TRUE,out.width=".7\\textwidth"}
some_ns <- seq(10,800,by=10)
pow_by_n <- sapply(some_ns, function(then){
    pwr.t.test(n = then, d = 0.25, sig.level = 0.05)$power
            })
plot(some_ns,pow_by_n,
    xlab="Sample Size",
    ylab="Power")
abline(h=.8)
## See https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html
## for fancier plots
## ptest <-  pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, power = .8)
## plot(ptest)
```


## Power by treatment effect size {.allowframebreaks}

```{r powplot_by_tau, echo=TRUE,out.width=".7\\textwidth"}
some_taus <- seq(0,1,by=.05)
pow_by_tau <- sapply(some_taus, function(thetau){
    pwr.t.test(n = 200, d = thetau, sig.level = 0.05)$power
            })
plot(some_taus,pow_by_tau,
    xlab="Average Treatment Effect (Standardized)",
    ylab="Power")
abline(h=.8)
```



## EGAP Power Calculator

- The calculator at: https://egap.shinyapps.io/power-app/

- For cluster randomization designs, try adjusting:

  - Number of clusters
  - Number of units per clusters
  - Intra-cluster correlation
  - Treatment effect


## Comments

- Know your outcome variable: what drives its variation regardless of treatment?

- What effects can you realistically expect from your treatment? What effects
  are substantively too small? (It may not be worth running *this experiment*
  at *this moment* if you cannot imagine detecting an effect at least this
  large.)

- What is the plausible range of variation of the outcome variable?

    - A design with limited possible movement in the outcome variable may not
      be well-powered.
    - See [10 Things Your Null Result Might Mean](https://egap.org/resource/10-things-your-null-result-might-mean/)
      for discussion of the various reasons a given experiment might have
      produced a $p > .05$ (if you are using $\alpha = .05$ as a rejection
      criteria or if you have some substantively small $\hat{\bar{\tau}}$ that
      might not be reached).


## Conclusion: How to improve your power


1. Increase the $N$

    - If clustered, increase the number of clusters if at all possible

2. Strengthen the treatment effect ($\tau_i$ and/or $\bar{\tau}$) (What might this mean in your study?)

3. Improve precision by removing extraneous noise from the outcome

    - Covariate adjustment

    - Blocking

4. Better measurement of the outcome variable (for example, using indicies to reduce noise in the outcome variable)


# Pre-Registration of Analysis Plans

## Bias in published research against null results {.allowframebreaks}

A good design executed well will produce credible research, which might be a
  null result.  We want credible and actionable null results.

- Manuscripts with
  null results are never submitted for review or put away in a "file drawer"
  after rejections.

- We face incentives to change your specifications, measurements, or even
  hypotheses to get a statistically significant result ($p$-hacking and HARKing) to improve
  chances of publication.
   - $p$-hacking: Trying many hypothesis tests increases the chances of a false positive result. (See Class 2 on Hythesis Testing Slides)
   - HARKing: (Hypotheses written After Results Known) Pretending that you are **testing** hypotheses when you are not (you are generating them --- which is fine but a separate activity).

- Even people not facing these incentives make many decisions when they analyze
  data: handling missing values and duplicate observations, creating scales,
  etc. Different seemingly small decisions can produce substantively meaningful
  differences in published results.

- Overall result:  reduced credibility for individual pieces of research and
  (rightly) reduced confidence in whether we actually know what we claim to
  know.

## Pre-registration of analysis plans and research designs I

- Pre-registration is the filing of your research design and hypotheses with a
  publicly-accessible repository with a credible date stamp.  EGAP hosts one
  that you can use for free.  Same with OSF. (EGAP registry lives within the
  larger OSF registry). See also the [OES Analysis Plans](https://oes.gsa.gov/work/).

- **Pre-registration does not preclude later exploratory analyses that were not
  stated in advance.**  You just have to clearly distinguish between the two.

## Pre-registration of analysis plans and research designs II

- Even if you will be submitting a paper with results to
  an academic journal or you are primarily interested in a final report with findings
  for a policy audience, there are important advantages to you and to other
  researchers from pre-registering your research.

   - You can learn about other research, completed and in progress; others can
     learn about yours.  We can learn about studies that produced null results.

   - It forces you to state your hypotheses and plan of analysis in advance of
     seeing the results, which limits $p$-hacking and HARKing.

# References

## References {.allowframebreaks}
